{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"15H8WVlc7Nb9qP1Zhbkc_UXbF_6H87efm","timestamp":1742367722096}],"collapsed_sections":["39xDiR3Pd0ha","z7G5vssVdsCx","A2ZhZz02mbkj","AT8mpQYzShjv","Ll8Gm3zqAkMX"],"gpuType":"T4","authorship_tag":"ABX9TyPAVoLG7giHl9z85K9kBIMo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 1. Install"],"metadata":{"id":"39xDiR3Pd0ha"}},{"cell_type":"code","source":["!pip install pydub speechrecognition\n","!apt-get install ffmpeg -y\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XjYbBFwDdpLT","executionInfo":{"status":"ok","timestamp":1742470427834,"user_tz":-60,"elapsed":18073,"user":{"displayName":"Slawo Lenc","userId":"06674288482359288787"}},"outputId":"09f1f75b-daed-4a86-e99f-cd117ed483a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting speechrecognition\n","  Downloading SpeechRecognition-3.14.1-py3-none-any.whl.metadata (31 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from speechrecognition) (4.12.2)\n","Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Downloading SpeechRecognition-3.14.1-py3-none-any.whl (32.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pydub, speechrecognition\n","Successfully installed pydub-0.25.1 speechrecognition-3.14.1\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n"]}]},{"cell_type":"code","source":["!pip install openai-whisper\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Hjzve5YmN-G","executionInfo":{"status":"ok","timestamp":1742470543375,"user_tz":-60,"elapsed":115530,"user":{"displayName":"Slawo Lenc","userId":"06674288482359288787"}},"outputId":"9695e80d-2b99-4826-bbb0-0681c3732d58"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai-whisper\n","  Downloading openai-whisper-20240930.tar.gz (800 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/800.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n","Collecting tiktoken (from openai-whisper)\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n","Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: openai-whisper\n","  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803406 sha256=e731cd881de13e36267674f1e6799ae514b7cf5cf560649ef827fff878eed55c\n","  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n","Successfully built openai-whisper\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n"]}]},{"cell_type":"markdown","source":["## 2.Google drive"],"metadata":{"id":"z7G5vssVdsCx"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"cS-f4SbfdpIN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742470626002,"user_tz":-60,"elapsed":82623,"user":{"displayName":"Slawo Lenc","userId":"06674288482359288787"}},"outputId":"cdf0bb46-5143-46b4-95e2-5829ae3b5d36"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## 3. Whisper used with  mp3\n"],"metadata":{"id":"A2ZhZz02mbkj"}},{"cell_type":"code","source":["import os\n","from pydub import AudioSegment\n","import whisper\n","\n","def mp3_to_wav(input_path):\n","    \"\"\"Converts MP3 file to WAV format and returns the path of the temporary WAV file.\"\"\"\n","    if not os.path.exists(input_path):\n","        raise FileNotFoundError(f\"The file {input_path} does not exist.\")\n","\n","    temp_wav = 'temp_audio.wav'\n","    audio_segment = AudioSegment.from_mp3(input_path)\n","    audio_segment.export(temp_wav, format='wav')\n","    return temp_wav\n","\n","def transcribe_audio(audio_file):\n","    \"\"\"Transcribes audio from the given file path using Whisper.\"\"\"\n","    try:\n","        model = whisper.load_model(\"base\")  # You can use \"small\", \"medium\", or \"large\" for better accuracy\n","        result = model.transcribe(audio_file)\n","        return result[\"text\"]\n","    except Exception as e:\n","        print(f\"Error during transcription: {e}\")\n","        return None\n","\n","def run_mp3_to_txt(input_path):\n","    # input_path = input(\"Enter the path to your MP3 file: \")\n","\n","    try:\n","        # Convert MP3 to WAV\n","        temp_wav = mp3_to_wav(input_path)\n","\n","        # Transcribe audio using Whisper\n","        text = transcribe_audio(temp_wav)\n","\n","        if text is not None:\n","            output_path = os.path.splitext(input_path)[0] + '_transcription.txt'\n","            with open(output_path, 'w', encoding='utf-8') as f:\n","                f.write(text)\n","            print(f\"Transcription saved to {output_path}\")\n","        else:\n","            print(\"No transcription was generated. Please check the audio file.\")\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n"],"metadata":{"id":"RRM1AVMwdpus"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# run_mp3_to_txt()"],"metadata":{"id":"ldtn9JusmEWD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CrmVb0u8m7WJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3mRBDUlzuHpc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742470791661,"user_tz":-60,"elapsed":131113,"user":{"displayName":"Slawo Lenc","userId":"06674288482359288787"}},"outputId":"2e249a42-ea18-4433-cdfd-b1adc863985b"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 210MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Transcription saved to /content/drive/MyDrive/mp3_to_md_whisper/2025-03-20-11-54-0ProteinEngineeringAIandResponsibleInnovation_transcription.txt\n"]}],"source":["#@title Transcribe mp3 to txt with whisper { display-mode: \"form\" }\n","# Data = \"data_dst\" #@param [\"data_src\", \"data_dst\"]\n","input_path = \"/content/drive/MyDrive/mp3_to_md_whisper/2025-03-20-11-54-0ProteinEngineeringAIandResponsibleInnovation.mp3\" #@param [\"\"] {allow-input: true}\n","\n","# print(fade_param)\n","# # fade_param\n","\n","run_mp3_to_txt(input_path)"]},{"cell_type":"markdown","source":["## 3 Reading txt file and translating from any language any language (50 possible)"],"metadata":{"id":"AT8mpQYzShjv"}},{"cell_type":"code","source":["from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n","import jieba\n","import chardet\n","import os\n","from tqdm import tqdm  # Import tqdm for progress bar\n","import torch  # PyTorch for GPU support\n","\n","def translate_text(file_path, source_lang='zh_CN', target_lang='en_XX', max_length=512):\n","    # Check if GPU is available and set device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")  # Show if using GPU or CPU\n","\n","    # Load model and tokenizer\n","    model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\").to(device)\n","    tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n","\n","    try:\n","        # Detect encoding\n","        with open(file_path, 'rb') as file:\n","            raw_data = file.read()\n","            result = chardet.detect(raw_data)\n","            encoding = result['encoding'] if result['confidence'] > 0.8 else 'utf-8'\n","\n","        # Read the text from the file\n","        with open(file_path, 'r', encoding=encoding) as file:\n","            text = file.read()\n","\n","        # Apply segmentation using jieba for better tokenization\n","        segmented_text = \" \".join(jieba.cut(text))\n","\n","        # Split text into manageable chunks if too long\n","        if len(segmented_text) > max_length:\n","            print(\"⚠️ Text too long; splitting into smaller chunks for translation.\")\n","            text_chunks = [segmented_text[i:i + max_length] for i in range(0, len(segmented_text), max_length)]\n","        else:\n","            text_chunks = [segmented_text]\n","\n","        # Create the output file path with language-specific prefix\n","        directory, filename = os.path.split(file_path)\n","        new_filename = f\"_{source_lang}_{target_lang}_{filename}\"\n","        output_path = os.path.join(directory, new_filename)\n","\n","        # Initialize list to store translations\n","        translations = []\n","\n","        # Use tqdm to show progress as we process the chunks\n","        print(f\"Processing {len(text_chunks)} chunks...\")\n","\n","        # Iterate over chunks with tqdm for a progress bar\n","        for chunk in tqdm(text_chunks, desc=\"Translating chunks\", unit=\"chunk\"):\n","            tokenizer.src_lang = source_lang\n","            encoded_text = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n","            generated_tokens = model.generate(\n","                **encoded_text,\n","                forced_bos_token_id=tokenizer.lang_code_to_id[target_lang],\n","                max_length=512\n","            )\n","            translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n","            translations.append(translation)\n","\n","        # Save the translation to the output file\n","        with open(output_path, 'w', encoding='utf-8') as output_file:\n","            output_file.write(\" \".join(translations))\n","\n","        return f\"Translation saved to {output_path}\"\n","\n","    except FileNotFoundError:\n","        return \"File not found.\"\n","    except Exception as e:\n","        return f\"An error occurred: {e}\"\n","\n","# # Example usage\n","# result = translate_text('/content/drive/MyDrive/mp3_to_md_whisper/biomap-2025-03-18-08-23-15_transcription.txt')\n","# print(result)\n"],"metadata":{"id":"HGMm-6VRfaGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7gODLYe6ipAh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Any to any translation"],"metadata":{"id":"Ll8Gm3zqAkMX"}},{"cell_type":"code","source":["# Arabic (ar_AR), Czech (cs_CZ), German (de_DE), English (en_XX), Spanish (es_XX), Estonian (et_EE), Finnish (fi_FI), French (fr_XX), Gujarati (gu_IN), Hindi (hi_IN), Italian (it_IT), Japanese (ja_XX), Kazakh (kk_KZ), Korean (ko_KR), Lithuanian (lt_LT), Latvian (lv_LV), Burmese (my_MM), Nepali (ne_NP), Dutch (nl_XX), Romanian (ro_RO), Russian (ru_RU), Sinhala (si_LK), Turkish (tr_TR), Vietnamese (vi_VN), Chinese (zh_CN), Afrikaans (af_ZA), Azerbaijani (az_AZ), Bengali (bn_IN), Persian (fa_IR), Hebrew (he_IL), Croatian (hr_HR), Indonesian (id_ID), Georgian (ka_GE), Khmer (km_KH), Macedonian (mk_MK), Malayalam (ml_IN), Mongolian (mn_MN), Marathi (mr_IN), Polish (pl_PL), Pashto (ps_AF), Portuguese (pt_XX), Swedish (sv_SE), Swahili (sw_KE), Tamil (ta_IN), Telugu (te_IN), Thai (th_TH), Tagalog (tl_XX), Ukrainian (uk_UA), Urdu (ur_PK), Xhosa (xh_ZA), Galician (gl_ES), Slovene (sl_SI)\n"],"metadata":{"id":"r5dxEMhIio9r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"kEMwNkFiA2AP"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1742466776159,"user_tz":-60,"elapsed":32235,"user":{"displayName":"Slawo Lenc","userId":"06674288482359288787"}},"outputId":"80f431f7-bfeb-438a-ab56-8403c83f0905","id":"o-tDKqaf8lUU"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","⚠️ Text too long; splitting into smaller chunks for translation.\n","Processing 7 chunks...\n"]},{"output_type":"stream","name":"stderr","text":["Translating chunks:   0%|          | 0/7 [00:00<?, ?chunk/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","Translating chunks: 100%|██████████| 7/7 [00:16<00:00,  2.30s/chunk]\n"]},{"output_type":"execute_result","data":{"text/plain":["'Translation saved to /content/drive/MyDrive/mp3_to_md_whisper/_fr_XX_pl_PL_fr2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}],"source":["#@title reading txt fiel and translating from  any language t other language{ display-mode: \"form\" }\n","# Data = \"data_dst\" #@param [\"data_src\", \"data_dst\"]\n","input_txt_path = \"/content/drive/MyDrive/mp3_to_md_whisper/fr2\" #@param [\"\"] {allow-input: true}\n","source_lang = \"fr_XX\" # @param [\"ar_AR\", \"cs_CZ\", \"de_DE\", \"en_XX\", \"es_XX\", \"et_EE\", \"fi_FI\", \"fr_XX\", \"gu_IN\", \"hi_IN\", \"it_IT\", \"ja_XX\", \"kk_KZ\", \"ko_KR\", \"lt_LT\", \"lv_LV\", \"my_MM\", \"ne_NP\", \"nl_XX\", \"ro_RO\", \"ru_RU\", \"si_LK\", \"tr_TR\", \"vi_VN\", \"zh_CN\", \"af_ZA\", \"az_AZ\", \"bn_IN\", \"fa_IR\", \"he_IL\", \"hr_HR\", \"id_ID\", \"ka_GE\", \"km_KH\", \"mk_MK\", \"ml_IN\", \"mn_MN\", \"mr_IN\", \"pl_PL\", \"ps_AF\", \"pt_XX\", \"sv_SE\", \"sw_KE\", \"ta_IN\", \"te_IN\", \"th_TH\", \"tl_XX\", \"uk_UA\", \"ur_PK\", \"xh_ZA\", \"gl_ES\", \"sl_SI\"]\n","# {\"allow-input\":true}\n","# target_lang = \"pl_PL\" # @param [\"pl_PL\",\"en_XX\"] {\"allow-input\":true}\n","target_lang = \"pl_PL\" # @param [\"ar_AR\", \"cs_CZ\", \"de_DE\", \"en_XX\", \"es_XX\", \"et_EE\", \"fi_FI\", \"fr_XX\", \"gu_IN\", \"hi_IN\", \"it_IT\", \"ja_XX\", \"kk_KZ\", \"ko_KR\", \"lt_LT\", \"lv_LV\", \"my_MM\", \"ne_NP\", \"nl_XX\", \"ro_RO\", \"ru_RU\", \"si_LK\", \"tr_TR\", \"vi_VN\", \"zh_CN\", \"af_ZA\", \"az_AZ\", \"bn_IN\", \"fa_IR\", \"he_IL\", \"hr_HR\", \"id_ID\", \"ka_GE\", \"km_KH\", \"mk_MK\", \"ml_IN\", \"mn_MN\", \"mr_IN\", \"pl_PL\", \"ps_AF\", \"pt_XX\", \"sv_SE\", \"sw_KE\", \"ta_IN\", \"te_IN\", \"th_TH\", \"tl_XX\", \"uk_UA\", \"ur_PK\", \"xh_ZA\", \"gl_ES\", \"sl_SI\"]\n","# {\"allow-input\":true}\n","# output_txt_path\n","translate_text(input_txt_path, source_lang, target_lang, max_length=512)"]},{"cell_type":"code","source":[],"metadata":{"id":"CQElC74uio6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Rla84O28C4B-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"r2mLoZe4C3-m"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0C7-4TI7SwwB"},"outputs":[],"source":["#@title reading txt fiel and translating from chinese to  other language{ display-mode: \"form\" }\n","# Data = \"data_dst\" #@param [\"data_src\", \"data_dst\"]\n","input_txt_path = \"/content/drive/MyDrive/mp3_to_md_whisper/biomap-2025-03-18-08-23-15_transcription.txt\" #@param [\"\"] {allow-input: true}\n","\n","# output_txt_path\n","translate_text(input_txt_path, source_lang='zh_CN', target_lang='en_XX', max_length=512)"]},{"cell_type":"code","source":[],"metadata":{"id":"mMJS3nFjffcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E_TMKkcoffZZ"},"execution_count":null,"outputs":[]}]}